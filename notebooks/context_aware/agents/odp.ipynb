{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f83bd59",
   "metadata": {},
   "source": [
    "# ODP\n",
    "\n",
    "By keeping a proper map of what has been explored, and using our knowledge of the world physics, we manage to get a more efficient algorithm in our \"Q-explorer\". Here we will try to take even more advantage of this information. \n",
    "\n",
    "If we think about the problem at hand, it is pretty clear that the agent is not exploring in an efficient way. For instance, if we already managed to traverse our world in $n$ steps, then why should the agent care to explore any state that is at a $n+1$ distance(or greater) from the origin? any path going throw such state will necessarily be longer. More generally we should want our agent only to consider going throw states that can lead to shorter paths.\n",
    "\n",
    "It may seem that this would lead to a very complicated exploration policy, however we can use our dynamic programming tools, to make this a really simple problem. Supose the agent has already completed a run of the world, so he know where the final state is, as well as some of the walls and traps. This agent can generate an \"optimistic\" map of the world, i.e. a map where each unknown state is considered to be empty(which is the best case sceneario for us), and then use this map and dynamic programming to \"plan\" a route to the final state. This route is the best possible route we can have, if the only caviat that it may not work, since while traversing this route the agent can hit a wall or a trap whenever it enters an unvisited state. However whenever this happens the agent can simply update his map and his policy(again by using dynamic programming). Here is the algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">   \n",
    "    <ul>\n",
    "        <li> keep track of each visited state, walls traps and the final state whenever we tack an action </li>\n",
    "        <li> use a generic exploration policy to complete a first run of the world </li>\n",
    "        <li> loop until agent follows a planned path without incidents </li>\n",
    "        <ul style=\"padding-bottom: 0;\">\n",
    "            <li> use the information gathered to generate an \"optimistic\" map of the world </li>\n",
    "            <li> use dynamic programming to determine a policy in our \"optimistic\" map </li>\n",
    "            <li> follow the policy. If something \"unexpected\" happens(i.e. we encounter a state different from what we have in the \"optmistic\" map): </li>\n",
    "            <ul style=\"padding-bottom: 1;\">\n",
    "                <li> update map </li>\n",
    "                <li> use dynamic programming to update policy </li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "This a somewhat simple algorithm, however it has some nice properties. For instance it has a halt condition, where we are garanted to have an optimal path. It also doesn't make unecessary explorations, i.e. explore things that couldn't possibly help it improve its performance.\n",
    "\n",
    "One problem that we haven't adressed so far is the size of the optmistic world, since the agent might not know the size of the actual world. One simple way is to stabilish an \"upper bound\" where we know the optimal path would need to be inside. For instance if the agent completed the exploration run by visitin $n$ unique states, then the optimal path can't go farther than $n$ from the starting position. This means we could consider our optimistic world to have size $(2n+1, 2n+1)$ and have the starting position at the midle. This is by no means an optimal estimation, but it would be enough to guarantee a solution. In our case however, just to make things simple, we will pass the size of the optmistic world as paramaters to our agent.\n",
    "\n",
    "Since this algorithm is pretty different from others this notebook will be a step by step implementation of it(similar in spirit to the one in the code base). We will analyze its performance in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfc0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../../..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from grid_world.action import Action\n",
    "from grid_world.grid_world import GridWorld\n",
    "from grid_world.agents.q_explorer_agent import QExplorerAgent\n",
    "from grid_world.visualization.format_objects import get_policy_rec_str, get_policy_eval_str, get_world_str\n",
    "from grid_world.utils.returns import returns_from_reward\n",
    "from grid_world.utils.policy import get_policy_rec, get_random_policy, sample_action\n",
    "\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b5907fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3               \n",
      "\n",
      "2          █    \n",
      "\n",
      "1    █     ☠    \n",
      "\n",
      "0 ⚐  █        ✘ \n",
      "\n",
      "  0  1  2  3  4 \n"
     ]
    }
   ],
   "source": [
    "gworld = GridWorld(\n",
    "    grid_shape=(4,5), \n",
    "    terminal_states_coordinates=((0,4),),\n",
    "    walls_coordinates=((0,1), (1,1), (2,3)),\n",
    "    traps_coordinates=((1,3),),\n",
    ")\n",
    "print(get_world_str(gworld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe14c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programing.policy_improvement import dynamic_programing_gpi\n",
    "\n",
    "\n",
    "# lets make some restrictions on the available actions\n",
    "actions = [Action.up, Action.down, Action.left, Action.right]\n",
    "\n",
    "def r(effect):\n",
    "    if effect == -1:\n",
    "        return -100\n",
    "    elif effect == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "rewards_dict = {(s, a): r(gworld.take_action(s, a)[1]) for s in gworld.states for a in actions}\n",
    "rewards = lambda x, y: rewards_dict[(x, y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b831620",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "### discovery run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4d2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Collection\n",
    "\n",
    "from grid_world.action import Action\n",
    "from grid_world.grid_world import GridWorld\n",
    "from grid_world.state import State\n",
    "from grid_world.type_aliases import Policy, RewardFunction, Q\n",
    "from grid_world.utils.evaluators import best_q_value\n",
    "from grid_world.utils.policy import (\n",
    "    get_random_policy,\n",
    "    sample_action,\n",
    "    get_explorer_policy,\n",
    ")\n",
    "from grid_world.utils.returns import returns_from_reward\n",
    "from utils.operations import add_tuples\n",
    "from grid_world.agents.world_map import WorldMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ab8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        reward_function: RewardFunction,\n",
    "        actions: Collection[Action] = None,\n",
    "        policy: Policy = None,\n",
    "        gamma: float = 1,\n",
    "        alpha: float = 0.1,\n",
    "        epsilon: float = 0.1,\n",
    "    ):\n",
    "        self.reward_function: Final = reward_function\n",
    "        self.actions: Final = actions if actions is not None else tuple(Action)\n",
    "        self.policy = Policy if policy is not None else get_random_policy(self.actions)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.world_map: set[State] = set()\n",
    "            \n",
    "    def update_world_map(self, state, action, new_state):\n",
    "        if new_state == state:\n",
    "            self.world_map.add(\n",
    "                State(add_tuples(state.coordinates, action.direction), \"wall\")\n",
    "            )\n",
    "        else:\n",
    "            self.world_map.add(new_state)\n",
    "\n",
    "    \n",
    "def run_random_episode(\n",
    "    agent, world, max_steps = 1000000\n",
    "):\n",
    "    \n",
    "    state = world.initial_state\n",
    "    episode_terminated = False\n",
    "    episode_states = [state]\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = sample_action(agent.policy, state, agent.actions)\n",
    "        new_state, effect = world.take_action(state, action)\n",
    "        reward = agent.reward_function(effect)\n",
    "        agent.update_world_map(state, action, new_state)\n",
    "\n",
    "        episode_actions.append(action)\n",
    "        episode_states.append(new_state)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if new_state.kind == \"terminal\":\n",
    "            episode_terminated = True\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "\n",
    "    return episode_terminated, episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c5b021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = BasicAgent(r, actions)\n",
    "episode_terminated, episode_states, episode_actions, episode_rewards = run_random_episode(agent, gworld)\n",
    "len(episode_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97db1e9",
   "metadata": {},
   "source": [
    "### determine optimistc world and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3052eee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 4),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_state_by_kind(kind, world_map, world_size):\n",
    "    return tuple(\n",
    "        a.coordinates for a in agent.world_map if (\n",
    "            a.kind == kind and all(0 <= x < world_size for x in a.coordinates)\n",
    "        )\n",
    "    )\n",
    "            \n",
    "get_state_by_kind(\"terminal\", agent.world_map, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70823e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5                  \n",
      "\n",
      "4    █             \n",
      "\n",
      "3                  \n",
      "\n",
      "2          █       \n",
      "\n",
      "1                  \n",
      "\n",
      "0 ⚐  █        ✘    \n",
      "\n",
      "  0  1  2  3  4  5 \n"
     ]
    }
   ],
   "source": [
    "def build_opt_world(world_size, agent):\n",
    "    return GridWorld(\n",
    "        grid_shape=(world_size, world_size), \n",
    "        terminal_states_coordinates=get_state_by_kind(\"terminal\", agent.world_map, world_size),\n",
    "        walls_coordinates=get_state_by_kind(\"wall\", agent.world_map, world_size),\n",
    "        traps_coordinates=get_state_by_kind(\"trap\", agent.world_map, world_size),\n",
    "    )\n",
    "optimistic_world = build_opt_world(6, agent)\n",
    "print(get_world_str(optimistic_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02bf677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy converged in 1 epochs\n"
     ]
    }
   ],
   "source": [
    "def get_world_model(world):\n",
    "    return lambda s, a: lambda x: 1 if x == world.take_action(s, a)[0] else 0\n",
    "\n",
    "def build_gpi_policy(world, r_map, actions):\n",
    "    world_model = get_world_model(world)\n",
    "    \n",
    "    rewards_dict = {(s, a): r(world.take_action(s, a)[1]) \n",
    "                    for s in world.states\n",
    "                    for a in actions\n",
    "                    }\n",
    "    rewards = lambda x, y: rewards_dict[(x, y)]\n",
    "\n",
    "    policy, _ = dynamic_programing_gpi(\n",
    "        world_model=world_model,\n",
    "        reward_function=rewards,\n",
    "        actions=actions,\n",
    "        states=world.states,\n",
    "    )\n",
    "    return policy\n",
    "\n",
    "policy = build_gpi_policy(optimistic_world, r, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d5e328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↓  →  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↓  █  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↓  ↓  ↓  →  ↓  ↓ \n",
      "\n",
      " ↓  ↓  ↓  █  ↓  ↓ \n",
      "\n",
      " →  →  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↑  █  →  →  ✘  ← \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_r = get_policy_rec(policy, optimistic_world, actions)\n",
    "print(get_policy_rec_str(pi_r, optimistic_world))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363e8ba",
   "metadata": {},
   "source": [
    "### improved run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d036c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_opt_episode(\n",
    "    agent, world, max_steps = 1000000\n",
    "):\n",
    "    \n",
    "    state = world.initial_state\n",
    "    episode_terminated = False\n",
    "    episode_states = [state]\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    \n",
    "    optimistic_world = build_opt_world(6, agent)\n",
    "    policy_rec = get_policy_rec(agent.policy, optimistic_world, agent.actions)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = policy_rec[state]\n",
    "        new_state, effect = world.take_action(state, action)\n",
    "        reward = agent.reward_function(effect)\n",
    "        agent.update_world_map(state, action, new_state)\n",
    "\n",
    "        episode_actions.append(action)\n",
    "        episode_states.append(new_state)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if new_state.kind == \"terminal\":\n",
    "            episode_terminated = True\n",
    "            break\n",
    "            \n",
    "        #check if policy is going well; if not we update our optimistic map, and then our policy\n",
    "        if new_state == state or new_state.kind == \"trap\":\n",
    "            optimistic_world = build_opt_world(6, agent)\n",
    "            agent.policy = build_gpi_policy(optimistic_world, r, actions)\n",
    "            policy_rec = get_policy_rec(agent.policy, optimistic_world, agent.actions)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    return episode_terminated, episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2445022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy converged in 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy = policy\n",
    "episode_terminated, episode_states, episode_actions, episode_rewards = run_opt_episode(agent, gworld)\n",
    "len(episode_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7e5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↓  →  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↓  █  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↓  ↓  ↓  →  ↓  ↓ \n",
      "\n",
      " →  →  ↓  █  ↓  ↓ \n",
      "\n",
      " ↑  █  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↑  █  →  →  ✘  ← \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimistic_world = build_opt_world(6, agent)\n",
    "pi_r = get_policy_rec(agent.policy, optimistic_world, actions)\n",
    "print(get_policy_rec_str(pi_r, optimistic_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2257a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↓  ↓  ↓  →  ↓ \n",
      "\n",
      " →  →  ↓  █  ↓ \n",
      "\n",
      " ↑  █  ↓  ☠  ↓ \n",
      "\n",
      " ↑  █  →  →  ✘ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_policy_rec_str(pi_r, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5816e9",
   "metadata": {},
   "source": [
    "## Second optimized run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc7a5a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_terminated, episode_states, episode_actions, episode_rewards = run_opt_episode(agent, gworld)\n",
    "len(episode_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b8d39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↓  →  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↓  █  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↓  ↓  ↓  →  ↓  ↓ \n",
      "\n",
      " →  →  ↓  █  ↓  ↓ \n",
      "\n",
      " ↑  █  ↓  ↓  ↓  ↓ \n",
      "\n",
      " ↑  █  →  →  ✘  ← \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimistic_world = build_opt_world(6, agent)\n",
    "pi_r = get_policy_rec(agent.policy, optimistic_world, actions)\n",
    "print(get_policy_rec_str(pi_r, optimistic_world))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2407baa",
   "metadata": {},
   "source": [
    "Note that althoug the agent has not find many of the walls and the trap, it has already find an optimal path(in only 2 runs!). So from here on it will just follow this path, without doing unecessary explorations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
