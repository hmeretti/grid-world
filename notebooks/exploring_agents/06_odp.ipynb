{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda7d2c5",
   "metadata": {},
   "source": [
    "# ODP\n",
    "\n",
    "By keeping a proper map of what has been explored, and using our knowledge of the world physics, we manage to get a more efficient algorithm in our \"Q-explorer\". Here we will take this to another level, and try to solve the problem as efficiently as we can. Just as with Q-explorer **we will assume a deterministic setting for this agent**.\n",
    "\n",
    "If we think about the problem at hand, it is pretty clear that the agent is not exploring in an efficient way. For instance, if we already managed to traverse our world in $n$ steps, then why should the agent care to explore any state that is at a $n+1$ distance(or greater) from the origin? any path going throw such state will necessarily be longer. More generally we should want our agent only to consider going throw states that can lead to shorter paths.\n",
    "\n",
    "It may seem that this would lead to a very complicated exploration policy, however we can use our dynamic programming tools, to make this a really simple problem. Supose the agent has already completed a run of the world, so he know where the final state is, as well as some of the walls and traps. This agent can generate an \"optimistic\" map of the world, i.e. a map where each unknown state is considered to be empty(which is the best case sceneario for us), and then use this map and dynamic programming to \"plan\" a route to the final state. This route is the best possible route we can have, with the only caviat that it may not work, since while traversing this route the agent can hit a wall or a trap whenever it enters an unvisited state. However whenever this happens the agent can simply update his map and his policy(again by using dynamic programming). Here is an outline of the algorithm.\n",
    "\n",
    "\n",
    "> <ul>\n",
    ">   <li> use a generic exploration policy to complete a first run of the world, and generate a map(wchich may be incomplete) </li>\n",
    ">   <li> use dynamic programming to determine a greedy policy in our map; unknown states are set as empty(the \"optimistic\" assumption) </li>\n",
    ">   <li> loop until agent passes an episode without incidents </li>\n",
    ">   <ul style=\"padding-bottom: 0;\">\n",
    ">        <li> follow the policy. If something \"unexpected\" happens(i.e. we encounter a state different from what we have in the \"optmistic\" map): </li>\n",
    ">        <ul style=\"padding-bottom: 1;\">\n",
    ">            <li> update map </li>\n",
    ">            <li> use dynamic programming to update policy </li>\n",
    ">        </ul>\n",
    ">    </ul>\n",
    "> </ul>\n",
    "\n",
    "\n",
    "This a somewhat simple algorithm, but it has some nice properties. For instance it has a halt condition, where we are guaranteed to have an optimal path. It also doesn't make unecessary explorations, i.e. explore things that couldn't possibly help it improve its performance.\n",
    "\n",
    "One problem that we haven't adressed so far is the size of the optmistic world, since the agent might not know the size of the actual world. One simple way is to estabilish an \"upper bound\" where we know the optimal path would be inside. For instance if the agent completed the exploration run by visitin $n$ unique states, then the optimal path can't go farther than $n$ from the starting position. This means we could consider our optimistic world to have size $(2n+1, 2n+1)$ and have the starting position at the midle. This is by no means an optimal estimation, but it would be enough to guarantee a solution. In our case however, just to make things simple, we will pass the size of the optmistic world as a paramaters to our agent.\n",
    "\n",
    "Another major drawback for this agent is how computationally expensive it is, since it has to solve multiple DP problemns to work.\n",
    "\n",
    "Since this algorithm is pretty different from others, this notebook will be a step by step implementation of it(similar in spirit to the implementation in the code base). We will look at its performance in other notebooks, but in truth there really isn't much to analyze, there are no hyperparameter or anything, it just kinda solves the problem(although there is definetily room for improvement, specially in the first episode).\n",
    "\n",
    "Extending this agent to a non-deterministic world could be simple, if we gave it the ability to determine the wind at any given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4490748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from exploring_agents.grid_world_agents import ODPAgent\n",
    "from exploring_agents.training import run_episode, train_agent\n",
    "from grid_world.action import Action\n",
    "from grid_world.grid_world import GridWorld\n",
    "from grid_world.visualization.format_objects import (\n",
    "    get_policy_rec_str,\n",
    "    get_policy_eval_str,\n",
    "    get_world_str,\n",
    ")\n",
    "from utils.returns import returns_from_reward\n",
    "from utils.policy import get_policy_rec, get_random_policy, sample_action\n",
    "from notebooks.utils.worlds import small_world_01\n",
    "from notebooks.utils.basics import basic_actions, basic_reward\n",
    "from dynamic_programing.policy_improvement import dynamic_programing_gpi\n",
    "\n",
    "\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9e24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ✘          \n",
      "\n",
      "3    ☠  █    \n",
      "\n",
      "2            \n",
      "\n",
      "1 █  █       \n",
      "\n",
      "0 ⚐          \n",
      "\n",
      "  0  1  2  3 \n"
     ]
    }
   ],
   "source": [
    "actions = basic_actions\n",
    "rewards = basic_reward\n",
    "gworld = small_world_01\n",
    "print(get_world_str(gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba89667",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "### discovery run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b81c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Collection\n",
    "\n",
    "from grid_world.action import Action\n",
    "from grid_world.grid_world import GridWorld\n",
    "from grid_world.state import State\n",
    "from grid_world.type_aliases import Policy, RewardFunction, Q\n",
    "from grid_world.utils.evaluators import best_q_value\n",
    "from grid_world.utils.policy import (\n",
    "    get_random_policy,\n",
    "    sample_action,\n",
    "    get_explorer_policy,\n",
    ")\n",
    "from grid_world.utils.returns import returns_from_reward\n",
    "from utils.operations import add_tuples\n",
    "from grid_world.agents.commons.world_map import WorldMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e851556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAgent:\n",
    "    def __init__(self, reward_function, actions=None):\n",
    "        self.reward_function: Final = reward_function\n",
    "        self.actions: Final = actions if actions is not None else tuple(Action)\n",
    "        self.world_map: set[State] = set()\n",
    "        self.policy = get_random_policy(self.actions)\n",
    "\n",
    "    def update_world_map(self, state, action, new_state):\n",
    "        if (new_state == state) and state.kind != \"terminal\":\n",
    "            self.world_map.add(\n",
    "                State(add_tuples(state.coordinates, action.direction), \"wall\")\n",
    "            )\n",
    "        else:\n",
    "            self.world_map.add(new_state)\n",
    "\n",
    "\n",
    "def run_random_episode(agent, world, max_steps=1000000):\n",
    "\n",
    "    state = world.initial_state\n",
    "    episode_terminated = False\n",
    "    episode_states = [state]\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = sample_action(agent.policy, state, agent.actions)\n",
    "        new_state, effect = world.take_action(state, action)\n",
    "        reward = agent.reward_function(effect)\n",
    "        agent.update_world_map(state, action, new_state)\n",
    "\n",
    "        episode_actions.append(action)\n",
    "        episode_states.append(new_state)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if new_state.kind == \"terminal\":\n",
    "            episode_terminated = True\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    return episode_terminated, episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5e8aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = BasicAgent(rewards, actions)\n",
    "(\n",
    "    episode_terminated,\n",
    "    episode_states,\n",
    "    episode_actions,\n",
    "    episode_rewards,\n",
    ") = run_random_episode(agent, gworld)\n",
    "len(episode_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86451c44",
   "metadata": {},
   "source": [
    "### determine optimistc world and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd5c373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 4),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_state_by_kind(kind, world_map, world_size):\n",
    "    return tuple(\n",
    "        a.coordinates\n",
    "        for a in agent.world_map\n",
    "        if (a.kind == kind and all(0 <= x < world_size for x in a.coordinates))\n",
    "    )\n",
    "\n",
    "\n",
    "get_state_by_kind(\"terminal\", agent.world_map, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89aba26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5       █          \n",
      "\n",
      "4 ✘                \n",
      "\n",
      "3       █     █    \n",
      "\n",
      "2             █    \n",
      "\n",
      "1 █  █        █    \n",
      "\n",
      "0 ⚐           █    \n",
      "\n",
      "  0  1  2  3  4  5 \n"
     ]
    }
   ],
   "source": [
    "def build_opt_world(world_size, agent):\n",
    "    return GridWorld(\n",
    "        grid_shape=(world_size, world_size),\n",
    "        terminal_states_coordinates=get_state_by_kind(\n",
    "            \"terminal\", agent.world_map, world_size\n",
    "        ),\n",
    "        walls_coordinates=get_state_by_kind(\"wall\", agent.world_map, world_size),\n",
    "        traps_coordinates=get_state_by_kind(\"trap\", agent.world_map, world_size),\n",
    "    )\n",
    "\n",
    "\n",
    "world_size = 6\n",
    "optimistic_world = build_opt_world(world_size, agent)\n",
    "print(get_world_str(optimistic_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61963a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_model(world):\n",
    "    return lambda s, a: lambda x: 1 if x == world.take_action(s, a)[0] else 0\n",
    "\n",
    "\n",
    "def build_gpi_policy(world, r_map, actions):\n",
    "    world_model = get_world_model(world)\n",
    "\n",
    "    rewards_dict = {\n",
    "        (s, a): r_map(world.take_action(s, a)[1]) for s in world.states for a in actions\n",
    "    }\n",
    "    rewards = lambda x, y: rewards_dict[(x, y)]\n",
    "\n",
    "    policy, _ = dynamic_programing_gpi(\n",
    "        world_model=world_model,\n",
    "        reward_function=rewards,\n",
    "        actions=actions,\n",
    "        states=world.states,\n",
    "    )\n",
    "    return policy\n",
    "\n",
    "\n",
    "policy = build_gpi_policy(optimistic_world, rewards, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a270bc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↓  ↓  █  ↓  ↓  ↓ \n",
      "\n",
      " ✘  ←  ←  ←  ←  ← \n",
      "\n",
      " ↑  ↑  █  ↑  █  ↑ \n",
      "\n",
      " ↑  ↑  ←  ↑  █  ↑ \n",
      "\n",
      " █  █  ↑  ↑  █  ↑ \n",
      "\n",
      " →  →  ↑  ↑  █  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_r = get_policy_rec(policy, optimistic_world, actions)\n",
    "print(get_policy_rec_str(pi_r, optimistic_world))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd564a",
   "metadata": {},
   "source": [
    "Notice that since the agent isn't aware of a trap at (1,3) it assumes it can go through this square, since it would lead to one of the shortest path. However this should be corrected on the next run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68698da7",
   "metadata": {},
   "source": [
    "### improved run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901eabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_opt_episode(agent, world, max_steps=1000000):\n",
    "\n",
    "    state = world.initial_state\n",
    "    episode_terminated = False\n",
    "    episode_states = [state]\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    optimistic_world = build_opt_world(world_size, agent)\n",
    "    policy_rec = get_policy_rec(agent.policy, optimistic_world, agent.actions)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = policy_rec[state]\n",
    "        new_state, effect = world.take_action(state, action)\n",
    "        reward = agent.reward_function(effect)\n",
    "        agent.update_world_map(state, action, new_state)\n",
    "\n",
    "        episode_actions.append(action)\n",
    "        episode_states.append(new_state)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if new_state.kind == \"terminal\":\n",
    "            episode_terminated = True\n",
    "            break\n",
    "\n",
    "        # check if policy is going well; if not we update our optimistic map, and then our policy\n",
    "        if new_state == state or new_state.kind == \"trap\":\n",
    "            optimistic_world = build_opt_world(world_size, agent)\n",
    "            agent.policy = build_gpi_policy(optimistic_world, rewards, actions)\n",
    "            policy_rec = get_policy_rec(agent.policy, optimistic_world, agent.actions)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    return episode_terminated, episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d16fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy = policy\n",
    "episode_terminated, episode_states, episode_actions, episode_rewards = run_opt_episode(\n",
    "    agent, gworld\n",
    ")\n",
    "len(episode_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d0d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↓  ↓  █  ↓  ↓  ↓ \n",
      "\n",
      " ✘  ←  ←  ←  ←  ← \n",
      "\n",
      " ↑  ☠  █  ↑  █  ↑ \n",
      "\n",
      " ↑  ←  ←  ↑  █  ↑ \n",
      "\n",
      " █  █  ↑  ↑  █  ↑ \n",
      "\n",
      " →  →  ↑  ↑  █  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimistic_world = build_opt_world(world_size, agent)\n",
    "pi_r = get_policy_rec(agent.policy, optimistic_world, actions)\n",
    "print(get_policy_rec_str(pi_r, optimistic_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04e7bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✘  ←  ←  ← \n",
      "\n",
      " ↑  ☠  █  ↑ \n",
      "\n",
      " ↑  ←  ←  ↑ \n",
      "\n",
      " █  █  ↑  ↑ \n",
      "\n",
      " →  →  ↑  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_policy_rec_str(pi_r, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018452c3",
   "metadata": {},
   "source": [
    "## Second optimized run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cad68d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_terminated, episode_states, episode_actions, episode_rewards = run_opt_episode(\n",
    "    agent, gworld\n",
    ")\n",
    "len(episode_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7d344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↓  ↓  █  ↓  ↓  ↓ \n",
      "\n",
      " ✘  ←  ←  ←  ←  ← \n",
      "\n",
      " ↑  ☠  █  ↑  █  ↑ \n",
      "\n",
      " ↑  ←  ←  ↑  █  ↑ \n",
      "\n",
      " █  █  ↑  ↑  █  ↑ \n",
      "\n",
      " →  →  ↑  ↑  █  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimistic_world = build_opt_world(world_size, agent)\n",
    "pi_r = get_policy_rec(agent.policy, optimistic_world, actions)\n",
    "print(get_policy_rec_str(pi_r, optimistic_world))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f485d",
   "metadata": {},
   "source": [
    "Note that althoug the agent has not find many of the walls and the trap, it has already find an optimal path(in only 2 runs!). So from here on it will just follow this path, without doing unecessary explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f60ba",
   "metadata": {},
   "source": [
    "# Codebase Agent\n",
    "\n",
    "lets take a quick look at the proper agent we implemented in the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5330fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 16, 9, 9, 9]\n",
      " ✘  ←  ←  ← \n",
      "\n",
      " ↑  ☠  █  ↑ \n",
      "\n",
      " ↑  ←  ←  ↑ \n",
      "\n",
      " █  █  ↑  ↑ \n",
      "\n",
      " →  →  ↑  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = ODPAgent(\n",
    "    reward_function=basic_reward, actions=basic_actions, world_shape=(6, 6)\n",
    ")\n",
    "\n",
    "\n",
    "episode_lengths, episode_returns = train_agent(agent=agent, world=gworld, episodes=5)\n",
    "pi_r = get_policy_rec(agent.policy, gworld, agent.actions)\n",
    "\n",
    "print(episode_lengths)\n",
    "print(get_policy_rec_str(pi_r, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2450f",
   "metadata": {},
   "source": [
    "If we pass the terminal states coordinates to the agent, it can do even better. It learns the path on the first run, which isn't very long in the first place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae6ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 9, 9, 9, 9]\n",
      " ✘  ←  ←  ← \n",
      "\n",
      " ↑  ☠  █  ↑ \n",
      "\n",
      " ↑  ←  ←  ↑ \n",
      "\n",
      " █  █  ↑  ↑ \n",
      "\n",
      " →  →  ↑  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = ODPAgent(\n",
    "    reward_function=basic_reward,\n",
    "    actions=basic_actions,\n",
    "    world_shape=(4, 5),\n",
    "    terminal_coordinates=(0, 4),\n",
    ")\n",
    "\n",
    "\n",
    "episode_lengths, episode_returns = train_agent(agent=agent, world=gworld, episodes=5)\n",
    "pi_r = get_policy_rec(agent.policy, gworld, agent.actions)\n",
    "\n",
    "print(episode_lengths)\n",
    "print(get_policy_rec_str(pi_r, gworld))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
