{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bf7ad1",
   "metadata": {},
   "source": [
    "# Wind World\n",
    "\n",
    "In this notebook, we will use dynamic programming to find optimal policies in a stochastic world. We will use 'wind' as a device to introduce some randomness; after each action, there is a chance the agent will be pushed either up or to the right by the wind.\n",
    "\n",
    "We will see that the same algorithm used in the deterministic setting works just fine, and also that different choice of rewards leads to different optimal policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df213eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dynamic_programing.policy_improvement import dynamic_programing_gpi\n",
    "from grid_world.grid_world import GridWorld\n",
    "from grid_world.visualization.format_objects import (\n",
    "    get_policy_rec_str,\n",
    "    get_policy_eval_str,\n",
    "    get_world_str,\n",
    ")\n",
    "from grid_world.utils.policy import get_policy_rec\n",
    "from grid_world.action import Action\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f104d1e",
   "metadata": {},
   "source": [
    "# Our World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c70d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ✘             \n",
      "\n",
      "4               \n",
      "\n",
      "3    ☠  █  █    \n",
      "\n",
      "2               \n",
      "\n",
      "1 █  █          \n",
      "\n",
      "0 ⚐             \n",
      "\n",
      "  0  1  2  3  4 \n"
     ]
    }
   ],
   "source": [
    "def wind(x: tuple[int, int]) -> Action:\n",
    "    n0 = np.random.uniform()\n",
    "    if n0 < 0.05:\n",
    "        return Action.right\n",
    "    elif n0 < 0.1:\n",
    "        return Action.up\n",
    "    else:\n",
    "        return Action.wait\n",
    "\n",
    "\n",
    "gworld = GridWorld(\n",
    "    grid_shape=(5, 6),\n",
    "    terminal_states_coordinates=((0, 5),),\n",
    "    walls_coordinates=((0, 1), (1, 1), (2, 3), (3, 3)),\n",
    "    traps_coordinates=((1, 3),),\n",
    "    wind=wind,\n",
    ")\n",
    "print(get_world_str(gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1b0ae",
   "metadata": {},
   "source": [
    "This is the world we will be considering, our goal is to reach the termial state as fast as possible, avoiding the trap. If this looks strange to you please refer to the readme file for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b487a9",
   "metadata": {},
   "source": [
    "# World Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fbdc2",
   "metadata": {},
   "source": [
    "Here we still need a model of the world to solve this problem with dynamic programming. Remember:\n",
    "\n",
    "$$ M_w: S \\times A \\to \\mathbb{P}(S) $$\n",
    "\n",
    "where $M_w$ gives for each pair state action $(s,a)$ a probability distribution over the states $S$, these indicate the probabilitie of moving to this new state when taking action $a$ in state $s$. This means that $M_w(s,a): S \\to [0, 1]$ is also a function and $M_w(s,a)(s_0)$ is the probability of getting to $s_0$ when taking action $a$ in state $s$. \n",
    "\n",
    "Since we are no longer in a deterministic setting we will estimate this values by sampling. For this we will be consulting the world freely, and estimating for each pair $(s,a)$ the probability of going to $s_0$. This is something an agent wouldn't be able to do in a reinforcement learning setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bb4369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04670000000000031"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [Action.up, Action.down, Action.left, Action.right]\n",
    "mw_dict = {}\n",
    "\n",
    "iterations_per_case = 10000\n",
    "increment = 1 / iterations_per_case\n",
    "for s in gworld.states:\n",
    "    for a in actions:\n",
    "        psa = {s0: 0 for s0 in gworld.states}\n",
    "        for _ in range(iterations_per_case):\n",
    "            fs = gworld.take_action(s, a)[0]\n",
    "            psa[fs] = psa[fs] + increment\n",
    "        mw_dict[(s, a)] = psa\n",
    "\n",
    "\n",
    "def world_model(s, a):\n",
    "    return lambda s0: mw_dict[(s, a)][s0]\n",
    "\n",
    "\n",
    "world_model(gworld.get_state((0, 0)), Action.up)(gworld.get_state((1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dbc30",
   "metadata": {},
   "source": [
    "# Rewards and Policy\n",
    "\n",
    "Lets define some reward functions and create some optimal policies through dynamic programing.\n",
    "\n",
    "I'll use the dynamic programing policy optmization algorithm implemented in the `dynamic_programing` module to avoid code repetition. It follows the exact same ideas from the determinitisc notebook.\n",
    "\n",
    "Lets start with the same reward from the deterministic notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07037a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(effect):\n",
    "    if effect == -1:\n",
    "        return -100\n",
    "    elif effect == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "rewards_dict = {\n",
    "    (s, a): r(gworld.take_action(s, a)[1]) for s in gworld.states for a in actions\n",
    "}\n",
    "\n",
    "\n",
    "def rewards(x, y):\n",
    "    return rewards_dict[(x, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347ada8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.00      -1.05      -2.10      -3.16      -4.21  \n",
      "\n",
      "    -1.05      -2.04      -3.05      -4.05      -5.06  \n",
      "\n",
      "    -2.06    -112.54                            -6.01  \n",
      "\n",
      "    -8.21      -8.96      -8.87      -7.91      -6.96  \n",
      "\n",
      "                          -9.77      -8.86      -7.92  \n",
      "\n",
      "   -12.54     -11.58     -10.68      -9.77      -8.87  \n",
      "\n",
      "\n",
      " ✘  ←  ←  ←  ← \n",
      "\n",
      " ↑  ←  ←  ←  ← \n",
      "\n",
      " ↑  ☠  █  █  ↑ \n",
      "\n",
      " ↑  ←  →  →  ↑ \n",
      "\n",
      " █  █  →  ↑  ↑ \n",
      "\n",
      " →  →  →  ↑  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi, v_pi = dynamic_programing_gpi(world_model, rewards, actions, gworld.states)\n",
    "pr = get_policy_rec(pi, gworld, actions)\n",
    "print(get_policy_eval_str(v_pi, gworld))\n",
    "print(get_policy_rec_str(pr, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485b63a",
   "metadata": {},
   "source": [
    "Here we see that the policy tries to avoid getting near the trap even though the path are expected to be longer(state (2,2) points right for instance). This is not surprising, as going down on left (2,2) or up on (0,2) will give a 5% chance of hitting the trap due to the wind.\n",
    "\n",
    "Now lets change the reward function, and see how this affects the policy.\n",
    "\n",
    "In particular we were giving a very negative reward for hitting the trap. We can change this, so that there is no extra punishiment for hitting the trap, excecpt that we are sent to the beggining.\n",
    "\n",
    "If we think of the problem like a video game, hiting the trap could mean something like losing a life. So the first reward function would be punishing this, while this second reward function doesn't care about it, and is just interested in reaching the terminal state as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd447a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_r(effect):\n",
    "    if effect == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "new_rewards_dict = {\n",
    "    (s, a): new_r(gworld.take_action(s, a)[1]) for s in gworld.states for a in actions\n",
    "}\n",
    "\n",
    "\n",
    "def new_rewards(x, y):\n",
    "    return new_rewards_dict[(x, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50514268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00     -1.05     -2.10     -3.16     -4.21  \n",
      "\n",
      "   -1.05     -2.04     -3.05     -4.05     -5.06  \n",
      "\n",
      "   -2.06    -10.80                         -6.01  \n",
      "\n",
      "   -3.42     -4.40     -5.79     -6.85     -6.96  \n",
      "\n",
      "                       -6.85     -7.84     -7.92  \n",
      "\n",
      "   -9.80     -8.85     -7.85     -8.79     -8.87  \n",
      "\n",
      "\n",
      " ✘  ←  ←  ←  ← \n",
      "\n",
      " ↑  ←  ←  ←  ← \n",
      "\n",
      " ↑  ☠  █  █  ↑ \n",
      "\n",
      " ↑  ←  ←  ←  ↑ \n",
      "\n",
      " █  █  ↑  ←  ↑ \n",
      "\n",
      " →  →  ↑  ↑  ↑ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_pi, new_v_pi = dynamic_programing_gpi(\n",
    "    world_model, new_rewards, actions, gworld.states\n",
    ")\n",
    "new_pr = get_policy_rec(new_pi, gworld, actions)\n",
    "print(get_policy_eval_str(new_v_pi, gworld))\n",
    "print(get_policy_rec_str(new_pr, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91326d6",
   "metadata": {},
   "source": [
    "We can see a change in the policy, now it prefers to take the shorter path. Even though following this policy has a reasonably high chance of hitting the trap due to the wind, on average this will be faster.\n",
    "\n",
    "The take away here is that the details of the reward function will influence the policy, and so we should always consider what we want to achieve and to avoid when defining then."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
