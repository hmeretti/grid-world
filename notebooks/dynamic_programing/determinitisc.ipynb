{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d36e715",
   "metadata": {},
   "source": [
    "# Dynamic Programing\n",
    "\n",
    "In this notebook, we will develop a dynamic programming solution to a grid world problem. By this I mean a function, called a policy: \n",
    "\n",
    "$$ \\pi : S \\times A \\to [0,1] $$\n",
    "\n",
    "that gives for each state the probability of choosing an action. A policy is optimal if by following this policy(i.e. choosing action $a$ in state $s$ with probability $ \\pi(s,a)$) we maximize the expected reward over an episode(here we can think of an episode starting at a random position and ending whenever we reach the terminal state).\n",
    "\n",
    "For this notebook I will consider a deterministic setting; all actions always have the same effect in any state. We will make free use of the information about the world to construct a model of the world's actions, which is required for dynamic programming. In this sense this not a reinforcement learning approach, as we will not have an agent interacting with the world to discover its behavior.\n",
    "\n",
    "Under these assumptions, the problem is pretty straightforward, and we will get an optimal solution by using an generalized policy iteration algorithm(GPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "included-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from grid_world.grid_world import GridWorld\n",
    "from grid_world.visualization.format_objects import (\n",
    "    get_policy_rec_str,\n",
    "    get_policy_eval_str,\n",
    "    get_world_str,\n",
    ")\n",
    "from grid_world.utils.policy import get_policy_rec\n",
    "from grid_world.action import Action\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cfe1b",
   "metadata": {},
   "source": [
    "# Our World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "still-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4                  \n",
      "\n",
      "3          █       \n",
      "\n",
      "2          █       \n",
      "\n",
      "1    █     ☠       \n",
      "\n",
      "0 ⚐  █           ✘ \n",
      "\n",
      "  0  1  2  3  4  5 \n"
     ]
    }
   ],
   "source": [
    "gworld = GridWorld(\n",
    "    grid_shape=(5, 6),\n",
    "    terminal_states_coordinates=((0, 5),),\n",
    "    walls_coordinates=((0, 1), (1, 1), (2, 3), (3, 3)),\n",
    "    traps_coordinates=((1, 3),),\n",
    ")\n",
    "print(get_world_str(gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc020e71",
   "metadata": {},
   "source": [
    "This is the world we will be considering, our goal is to reach the termial state as fast as possible, avoiding the trap. If this looks strange to you please refer to the readme file for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6d9f3",
   "metadata": {},
   "source": [
    "# World Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906828a0",
   "metadata": {},
   "source": [
    "In order to solve this problem with dynamic programming we will need a model of the world and a reward function. Mathematicaly these are the functions we need:\n",
    "\n",
    "$$ M_w: S \\times A \\to \\mathbb{P}(S) $$\n",
    "$$ R_w: S \\times A \\to \\mathbb{R} $$\n",
    "\n",
    "where $M_w$ gives for each pair state action $(s,a)$ a probability distribution over the states $S$, these indicate the probabilitie of moving to this new state when taking action $a$ in state $s$. This means that $M_w(s,a): S \\to [0, 1]$ is also a function and $M_w(s,a)(s_0)$ is the probability of getting to $s_0$ when taking action $a$ in state $s$. Since we are in a determinitisct setting these values will be either 0 or 1.\n",
    "\n",
    "On the hand, $R_w(s,a)$ is the reward we get for taking action $a$ in state $s$. This is something we need to choose by ourselves. Since we want to reach the terminal state as soon as possible we will add a negative reward whenever we take an action outside it, for flavor we will also a big negative reward for being inside the trap. There is no reward for being in the terminal state since this should indicate the end of an episode(this is necessary for our implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e274e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make some restrictions on the available actions\n",
    "actions = [Action.up, Action.down, Action.left, Action.right]\n",
    "\n",
    "\n",
    "def r(effect):\n",
    "    if effect == -1:\n",
    "        return -100\n",
    "    elif effect == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "rewards_dict = {\n",
    "    (s, a): r(gworld.take_action(s, a)[1]) for s in gworld.states for a in actions\n",
    "}\n",
    "rewards = lambda x, y: rewards_dict[(x, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be487794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_model(s, a, world=gworld):\n",
    "    final_state = world.take_action(s, a)[0]\n",
    "    return lambda x: 1 if x == final_state else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba07b19",
   "metadata": {},
   "source": [
    "# Policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f73d9",
   "metadata": {},
   "source": [
    "Alright, the first step to implementing a dynamic programming solution is doing policy evaluation, this means that given a policy $\\pi$ we want to calculate a function, called the value function of $\\pi$:\n",
    "\n",
    "$$ V_{\\pi}: S \\to \\mathbb{R} $$\n",
    "\n",
    "that gives for each state $s$ the expected reward we will get until we reach a terminal state, when following our policy(usualy discounted by a $\\gamma$ factor which we will set to 1).\n",
    "\n",
    "There are many ways to calculate this, we will use an iterative approach that can be generalized to the reinforcement learning methods we want to explore. The idea is to start with a random value function $V$ and improve each state estimate like this:\n",
    "\n",
    "$$ V(s) \\leftarrow \\sum_{a \\in A}\\pi(s,a)\\sum_{s_0 \\in S}M_w(s,a)(s_0)(R(s,a) + \\gamma V(s_0))$$\n",
    "\n",
    "This is essentialy bootstrapping, for each state we of observe the reward for taking action $a$ and the estimated value of our new state, adding these gives an estimate of the value of this action in this state, so we average everything with the respective probability.\n",
    "\n",
    "Notice that, since $V(s_0)$ is expected to be wrong this new estimate can also be wrong, however now it incorporates information about the actual rewards, and by iterating this method $V$ actually converges to $V_\\pi$.\n",
    "\n",
    "When implementing this we will make $V$ a hashmap(a dictionary) since it is much more practical to update hash values then it is to change functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respected-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _acc_V(s, V, pi, world_model, reward_function, states, gamma):\n",
    "    return np.sum(\n",
    "        [\n",
    "            pi(s, a)\n",
    "            * np.sum(\n",
    "                [\n",
    "                    world_model(s, a)(s0) * (reward_function(s, a) + gamma * V[s0])\n",
    "                    for s0 in states\n",
    "                ]\n",
    "            )\n",
    "            for a in actions\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def _iterate_policy_step(pi, world_model, reward_function, actions, states, V_0, gamma):\n",
    "    V = V_0.copy()\n",
    "    for s in states:\n",
    "        V_0[s] = _acc_V(s, V, pi, world_model, reward_function, states, gamma)\n",
    "    return np.amax(np.abs([V_0[x] - V[x] for x in V_0]))\n",
    "\n",
    "\n",
    "def iterative_policy_evalution(\n",
    "    pi, world_model, reward_function, actions, states, V_0, epsilon=0.01, gamma=1\n",
    "):\n",
    "    delta = 2 * epsilon\n",
    "    while delta > epsilon:\n",
    "        delta = _iterate_policy_step(\n",
    "            pi, world_model, reward_function, actions, states, V_0, gamma\n",
    "        )\n",
    "\n",
    "    return V_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af7321",
   "metadata": {},
   "source": [
    "Lets test this on a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "written-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -773.16    -761.50    -731.95    -662.40    -588.89    -556.93  \n",
      "\n",
      "  -780.86    -775.42    -767.99               -543.35    -521.00  \n",
      "\n",
      "  -790.05    -787.36    -792.64               -516.19    -458.75  \n",
      "\n",
      "  -797.97               -818.60    -901.92    -542.50    -335.08  \n",
      "\n",
      "  -801.93               -757.28    -692.00    -412.83       0.00  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pi(s, a):\n",
    "    return 0.25\n",
    "\n",
    "\n",
    "V_0 = {x: 0 for x in gworld.states}\n",
    "V_pi = iterative_policy_evalution(pi, world_model, rewards, actions, gworld.states, V_0)\n",
    "print(get_policy_eval_str(V_pi, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d0aba",
   "metadata": {},
   "source": [
    "It's pretty hard to tell whether this is right or not, but it will get clearer latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d658b96",
   "metadata": {},
   "source": [
    "# Policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfb9d1",
   "metadata": {},
   "source": [
    "The other corner stone of GPI is to improve the policy, this is pretty obvious, once we have values for each state we simply make a policy that will send us to the state with better value; this is called a greedy policy with repect to $V$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eleven-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(s, a, V, world_model, reward_function, states):\n",
    "    return reward_function(s, a) + np.sum(\n",
    "        [world_model(s, a)(s0) * V[s0] for s0 in states]\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: function needs improvement\n",
    "def _argmax_q(s, V, world_model, reward_function, actions, states):\n",
    "    best_score = q(s, actions[0], V, world_model, reward_function, states)\n",
    "    best_action = actions[0]\n",
    "    for a in actions:\n",
    "        qa = q(s, a, V, world_model, reward_function, states)\n",
    "        if qa > best_score:\n",
    "            best_score = qa\n",
    "            best_action = a\n",
    "    return best_action\n",
    "\n",
    "\n",
    "def get_greedy_policy(V, world_model, reward_function, actions, states):\n",
    "    gpr = {\n",
    "        s: _argmax_q(s, V, world_model, reward_function, actions, states)\n",
    "        for s in states\n",
    "    }\n",
    "\n",
    "    return lambda s, a: 1 if (a == gpr[s]) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce7867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " →  →  →  →  ↓  ↓ \n",
      "\n",
      " ↑  ↑  ↑  █  ↓  ↓ \n",
      "\n",
      " ↑  ↑  ↑  █  →  ↓ \n",
      "\n",
      " ↑  █  ↓  ☠  →  ↓ \n",
      "\n",
      " ↑  █  →  →  →  ✘ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_1 = get_greedy_policy(V_pi, world_model, rewards, actions, gworld.states)\n",
    "pr = get_policy_rec(pi_1, gworld, actions)\n",
    "print(get_policy_rec_str(pr, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74153275",
   "metadata": {},
   "source": [
    "# GPI - Iterating till convergence\n",
    "\n",
    "Notice that since we change the policy to say $\\pi'$, $V$ may not be a good estimate of $V_{\\pi'}$, so $\\pi'$ is not necessarily greedy with respect to $V_{\\pi'}$. So the idea is to repeat the process until we get a policy that is greedy with respect to its value function; it is not hard to see that this is an optimal policy(check Sutton and Barto if you need).\n",
    "\n",
    "I will use changes in the value function as the stop criteria, since it is a little easier to check. This also guarantess that the policy is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b239598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      " →  →  →  →  ↓  ↓ \n",
      "\n",
      " ↑  ↑  ↑  █  ↓  ↓ \n",
      "\n",
      " ↑  ↑  ↑  █  →  ↓ \n",
      "\n",
      " ↑  █  ↓  ☠  →  ↓ \n",
      "\n",
      " ↑  █  →  →  →  ✘ \n",
      "\n",
      "\n",
      "epoch: 1\n",
      " →  →  →  →  ↓  ↓ \n",
      "\n",
      " ↑  ↑  ↑  █  ↓  ↓ \n",
      "\n",
      " ↑  ↑  ↓  █  ↓  ↓ \n",
      "\n",
      " ↑  █  ↓  ☠  ↓  ↓ \n",
      "\n",
      " ↑  █  →  →  →  ✘ \n",
      "\n",
      "\n",
      "epoch: 2\n",
      " →  →  →  →  ↓  ↓ \n",
      "\n",
      " ↑  ↑  ↓  █  ↓  ↓ \n",
      "\n",
      " ↑  →  ↓  █  ↓  ↓ \n",
      "\n",
      " ↑  █  ↓  ☠  ↓  ↓ \n",
      "\n",
      " ↑  █  →  →  →  ✘ \n",
      "\n",
      "\n",
      "epoch: 3\n",
      " →  →  ↓  →  ↓  ↓ \n",
      "\n",
      " ↑  ↓  ↓  █  ↓  ↓ \n",
      "\n",
      " →  →  ↓  █  ↓  ↓ \n",
      "\n",
      " ↑  █  ↓  ☠  ↓  ↓ \n",
      "\n",
      " ↑  █  →  →  →  ✘ \n",
      "\n",
      "\n",
      "epoch: 4\n",
      " →  ↓  ↓  →  ↓  ↓ \n",
      "\n",
      " ↓  ↓  ↓  █  ↓  ↓ \n",
      "\n",
      " →  →  ↓  █  ↓  ↓ \n",
      "\n",
      " ↑  █  ↓  ☠  ↓  ↓ \n",
      "\n",
      " ↑  █  →  →  →  ✘ \n",
      "\n",
      "\n",
      "policy convergerd\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "\n",
    "def float_dict_compare(d0, d1):\n",
    "    return np.all([np.isclose(d0[x], d1[x]) for x in d0])\n",
    "\n",
    "\n",
    "def dpi_step(V_pi, world_model, reward_function, actions, states):\n",
    "    pi_1 = get_greedy_policy(V_pi, world_model, reward_function, actions, states)\n",
    "    V_pi_1 = iterative_policy_evalution(\n",
    "        pi_1, world_model, reward_function, actions, states, V_pi\n",
    "    )\n",
    "\n",
    "    return pi_1, V_pi_1\n",
    "\n",
    "\n",
    "def pi(s, a):\n",
    "    return 0.25\n",
    "\n",
    "\n",
    "V_0 = {x: 0 for x in gworld.states}\n",
    "V_pi = iterative_policy_evalution(pi, world_model, rewards, actions, gworld.states, V_0)\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    V_pi_0 = V_pi.copy()\n",
    "    pi, V_pi = dpi_step(V_pi, world_model, rewards, actions, gworld.states)\n",
    "\n",
    "    if float_dict_compare(V_pi, V_pi_0):\n",
    "        print(\"policy convergerd\")\n",
    "        break\n",
    "\n",
    "    print(f\"epoch: {i}\")\n",
    "    pr = get_policy_rec(pi, gworld, actions)\n",
    "    print(get_policy_rec_str(pr, gworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e4f4a",
   "metadata": {},
   "source": [
    "Notice how the policy starts by avoiding the shorter path, which passes near the trap. This happens because the initial random policy has a chance of walking to the trap, and as a consequence the values of states near the trap start much lower then their values under the optimal policy. As we iterate these values get adjusted to the improved policies, and the improvements sorta propagates back to other states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
